# -*- coding: utf-8 -*-
"""facial recognition

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ggea9hYm2E6-z2umyJ2E4VZZtjvqur05
"""

pip install utils

pip install livelossplot

# Commented out IPython magic to ensure Python compatibility.
import utils
import pandas as pd
import numpy as np
import seaborn as sns
import os 
import matplotlib.pyplot as plt

# %matplotlib inline

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Conv2D
from tensorflow.keras.layers import BatchNormalization, Activation, MaxPooling2D
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.utils import plot_model
from IPython.display import SVG, Image
from livelossplot import PlotLossesKeras
from livelossplot.keras import PlotLossesCallback
import tensorflow as tf
print("Tensorflow version:", tf.__version__)

from google.colab import drive
drive.mount('/content/drive')

cd 'drive/My Drive'

ls

!unzip "/content/drive/My Drive/images_file.zip" -d "/content/drive/My Drive/images"

for expression in os.listdir("/content/drive/My Drive/images/images/train/"):
    print(str(len(os.listdir("/content/drive/My Drive/images/images/train/" + expression)))+  " " + expression + " " + "images")

for expression in os.listdir("/content/drive/My Drive/images/images/validation/"):
    print(str(len(os.listdir("/content/drive/My Drive/images/images/validation/" + expression)))+  " " + expression + " " + "images")

img_size = 48
batch_size = 64
datagen_train = ImageDataGenerator(horizontal_flip= True)
train_generator = datagen_train.flow_from_directory("/content/drive/My Drive/images/images/train/",
target_size = (img_size, img_size), color_mode= 'grayscale',batch_size= batch_size, 
class_mode = 'categorical', shuffle = True)


datagen_validation = ImageDataGenerator(horizontal_flip= True)
validation_generator = datagen_train.flow_from_directory("/content/drive/My Drive/images/images/validation/",
target_size = (img_size, img_size), color_mode= 'grayscale',batch_size= batch_size, 
class_mode = 'categorical', shuffle = False)

#conv layer 1
model = Sequential()
model.add(Conv2D(64,(3,3), padding='same', input_shape=(48,48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

#conv layer 2
model = Sequential()
model.add(Conv2D(128,(5,5), padding='same', input_shape=(48,48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

#conv layer 3
model = Sequential()
model.add(Conv2D(512,(3,3), padding='same', input_shape=(48,48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

#conv layer 4
model = Sequential()
model.add(Conv2D(64,(3,3), padding='same', input_shape=(48,48,1)))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

#flattening
model.add(Flatten())

# fully connected layer 1
model.add(Dense(256))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

#fully connected layer 2
model.add(Dense(512))
model.add(BatchNormalization())
model.add(Activation('relu'))
model.add(Dropout(0.25))

#output layer
model.add(Dense(7, activation= 'softmax'))

#optimizing the layers
opt = Adam(lr=0.0005)

#compilation 
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])

#summary
model.summary()

epochs =15
steps_per_epoch = train_generator.n//train_generator.batch_size
validation_steps = validation_generator.n//validation_generator.batch_size

checkpoint = ModelCheckpoint("model_weights.h5", monitor='val_accuracy',save_weights_only=True, 
mode='max', verbose=1)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,
patience=2, min_lr=0.00001, mode='auto')
callbacks = [PlotLossesKeras(), checkpoint, reduce_lr]
history = model.fit(
    x=train_generator,
    steps_per_epoch=steps_per_epoch,
    epochs=epochs,
    validation_data = validation_generator,
    validation_steps = validation_steps,
    callbacks=callbacks
)

